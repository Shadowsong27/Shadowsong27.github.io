+++
title = "Career"
date = "2021-02-14"
+++

This page should serve more like an informal introduction in my profession career and skill sets, if you are 
a recruiter, I would appreciate you to read this before you ask me for a CV.

I am a data engineer, I tend data pipelines and make data flows efficiently for my
fellow data analysts and data scientists.

You should only notice me when something is broken.

I am currently leading a team of three full time employees in building the data architecture for a startup. 

Besides data engineering, I do cover an extensive amount of analytical work as well, you know what they say
about wearing hats in startup.


# Education Background

### National University of Singapore - Aug 2014 to May 2018 

- Bachelor of Science in Business Analytics
- Science & Tech Scholarship

So I do hold a degree from a somewhat famous university in Asia, 
but nothing else really stands out academically, just a normal PRC scholar. 

# Work Experience

### Data Team Lead - May 2020 to now

(To be updated in details when I am open for job opportunities ...)

Main responsibilities:

- Handle the overall architectural design and initial implementation of data projects, including
data extraction in large scale, data processing and correction

- Weekly sprint planning, detailing of tasks

- Mentoring of teammates

- conduct research and build POC when new frameworks, tools are needed

### Data Engineer - Aug 2018 to May 2020

This role requires me to work both as a backend developer and a data engineer, 
it also requires me to communicate a lot with different stakeholders in 
different functional teams.

Main responsibilities:

Data Engineering

- Maintain (Python, SQL, AWS ECS) batch/stream data pipelines using Apache Airflow 
- Mgrated Data Warehouse from legacy Postgres RDS into AWS Redshift
- Design Data Warehouse (AWS Redshift) architecture, data health check and alerts (Emails and Slack) to speed up data analysis, validation, exploration & hypothesis testing
- Data modelling and design optimised analytical tables to speed up B2B data deliveries and data science iteration
- Enable safe automated data deployment into production environment and validation 

Backend & DevOps

- Handle several (Python Falcon and Django) backend services, including data science projects and internal services, deployed in containers (AWS Fargate)
- Implement a data back office to augment automatically processed data with manually cleaned data
- Setup CI/CD processes
- Build a few internal Python libraries shared across projects

Data Analysis

- Build and maintain monitoring dashboards (Metabase, Mode Analytics) for in-app usage, data quality and service performances
- Investigate data anomalies to help the team theorise  / make decisions
- Support marketing and product team with data insights

Data Science

- Maintain, refactor and debug existing data science projects
- Brainstorm improvements / implement based on user inputs and other data metrics


# Professional Skills

legend (being as accurate as I can):

- `Basic understanding`: Never used in production environment 
    but has a general idea on how a solution should be like, able to build POCs
- `familiar`: able to design, implement solutions
- `proficient`: able to design, implement and optimized solutions, workflow

### Engineering Skills

Programming language

- Main programming language is Python 3, 
    familiar with Python coding standard, common practices and design patterns
    
- Familiar with Python web backend frameworks 
    (Falcon and Django), familiar with RESTful API design

Data

- Proficient in Data modeling
- Proficient in Airflow, aware of pitfalls, able to design custom Plugins, able to device workarounds for framework related bugs
- Proficient in modern Data Warehouse solution design (Inmon approach), data pipeline designs
- Proficient Postgres (Redshift) SQL skill for both OLAP and OLTP types of queries, familiar with and relational database knowledge
- Familiar with other databases such as KV store, AWS Dynamo etc.
- Basic understanding with open sourced big data technologies including Kafka and Spark

General

- Familiar with modern programming toolkit: Unix terminal operations, version control, Docker, setup CI/CD, documentations
- Familiar in commonly used Amazon Web Services, able to independently deploy solutions from start to end


### Analytical Skills

- Able to understand and grasp the business context quickly
- Able to understand data, data flow, transform business requirements to technical requirements
- Able to temporarily cover data analysis requests and simple model building 
- Data visualisation

### Other Skills

- Fluent in mandarin and English
- Strong self-learning capability and a desire for knowledge
- Able to communicate with data analysts, data scientists and understand, accept / pushback / propose alternative solutions for their needs
- Able to communicate with non-data, non-technical team members (marketing, sales, upper management) 
- Experience with Team and Project Management (managed interns, managed JIRA boards)

